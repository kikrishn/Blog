---
layout: post
title: Rational Canonical Form
tags:
  - Linear Algebra
---

Let $n$ be a positive integer $\geq 1$. Let $V$ be an $n$ dimensional vector space over a field $F$, and $L(V,V)$ be the set of endomorphisms of $V$. The set $L(V,V)$ has a canonical structure of a vector space over $F$, and its dimension is $n^2$. 

Fix a nonzero element $T$ in $L(V,V)$. Then the set of endomorphisms $1, T, \ldots , T^{n^2}$ are linearly dependent over $F$, so that there exist elements $a_0, \ldots, a_{n^2}$ in $F$, not all zero, such that 

$$a_0 \cdot 1 + a_1 \cdot T + \ldots + a_{n^2} \cdot T^{n^2} = 0.$$

In other words, $T$ satisfies the polynomial $a_0 + a_1 t + \ldots + a_{n^2} t^{n^2} \in F[t]$. In general, the set of polynomials $f(X)$ in $F[X]$ such that $f(T) = 0$ forms an ideal in $F[t]$, and since $F[t]$ is a principal ideal domain, the ideal is generated by a monic polynomial $m(t)$, called the **minimal polynomial** of $T$. In other words, the polynomial $m(t)$ is the polynomial of least degree which $T$ satisfies and if $f(T) = 0$ then $m(t)$ divides $f(t)$. **Note that $m(t)$ is not necessarily irreducible over $F$.**

One more observation in order. The minimal polynomial $m(T)$ kills the entire space $V$. The first idea that might be expected to give additional information is to study not only polynomials $f(t)$ such that $f(T)$ annihilates $V$, but also polynomials in $T$ which send individual vectors to zero. The simplest case is that of a polynomial $t-\alpha$. That is, given $\alpha \in F$, we ask for vectors $v \in V$ such that $(T - \alpha)v = 0$, and we call $\alpha$ an eigenvalue of $T$ if there exists a nonzero vector $v \in V$ such that $T(v) = \alpha v$, and any nonzero vector satisfying this equation is called a **eigenvector** corresponding to the eigenvalue $\alpha$. 

**Example:** Consider the vector space $\mathcal{F}(\mathbb{R})$ of all functions $f: \mathbb{R} \to \mathbb{R}$, and $V$ be the subspace consisting of differentiable functions. Clearly the operation of differentiation defines a linear transformation 

$$d: V \to \mathcal{F}(\mathbb{R}).$$

A nonzero function $f$ is an eigenvector of $d$ if for some $\alpha \in \mathbb{R}$, we have $d(f) = \alpha f$. Clearly the exponential functions $e^{\alpha t}$ are the eigenvectors of $d$, and the theory of linear differential equations of the first order shows that these are the only eigenvectors of $d$.


<strong> Theorem </strong>: Eigenvectors belonging to distinct eigenvalues (of $T$) are linearly independent.

<strong> Theorem </strong> An element $\alpha \in F$ is an eigenvalue of $T$ if and only if $\textrm{det}(T - \alpha 1) = 0$, where $1$ is the identity transformation on $V$.

This result can be used to prove that the set $e^{\alpha_1 t}, \ldots, e^{\alpha_s t}$, where $\alpha_i$ are distinct, is linearly independent. Indeed, the differentiation operator takes be the subspace $V$ spanned by them to itself, so that $d$ restricts to a linear operator on $V$, and the above vectors (functions $e^{\alpha_i t} : \mathbb{R} \to \mathbb{R}$) are the eigenvectors corresponding to distinct eigenvalues $\alpha_1, \ldots, \alpha_s$ of $d_{V}: V \to V$.



There is yet another way to view eigenvectors. A nonzero vector $v \in V$ is an eigenvector of $T$ if and only if the one-dimensional subspace spanned by $v$ is invariant under $T$. In general, we may search for invariant subspaces of $T$ in $V$.

**Example**: Let $T$ be a linear transformation $\mathbb{R}^2$ such that for some basis $\{ v_1, v_2 \}$ of $\mathbb{R}^2$, 

$$
\begin{align}
T(v_1) &= v_2,\\
T(v_2) &= v_1.
\end{align}
$$

The eigenvectors of $T$ are $w_1 = v_1 + v_2$ and $w_2 = v_1 - v_2$ and the matrix of $T$ with respect to the basis ${w_1, w_2}$ is 
$$
A=\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}.
$$
We can now see that $T$ is a reflection with respect to the line through the origin in the direction of the vector $w_1$; it sends each vectors in $w_1$ onto itself and sends $w_2$ onto $-w_2$ with respect to the line $w_1$.  This concept leads to the notion of **$T$-invariant subspace**, namely $W$ is a <strong> $T$-invariant subspace </strong> of $V$ if $Tw \in W$ for all $w \in W$. 

Given $f(t) \in F[t]$, the null space of $f(T)$, namely 
$$
{ v \in V | f(T)v = 0 }
$$
is a $T$-invariant subspace. In the previous example $w_1$ generates the null space corresponding to the polynomial $t - 1$ and $w_2$ generates the null space corresponding to the polynomial $t + 1$. The polynomials $t + 1$ and $t-1$ are exactly the irreducible factors of the minimal polynomial $x^2 - 1$ of $T$. We will see below that this is a special instance of the theorem we intend to exhibit. First, some definitions.

The vector space $V$ is said to be <strong>direct sum</strong> of subspaces $V_1, \ldots, V_s$,  if every vector in $V$ is a unique sum of vectors in $V_1, \ldots, V_s$. Equivalently, if every vector $v$ in $V$ can be expressed as 
$$
v = v_1 + \ldots + v_s, \hspace{0.5cm} v_i \in V_i
$$
and if a sum $v_1 + \ldots + v_s$ is zero then each $v_i$ is zero. We have the following useful criterion to verify whether a vector space can be expressed as a direct sum.

<strong> Lemma </strong> Suppose there exist nonzero linear maps $E_1, \ldots, E_s$ in $L(V,V)$ such that their sum is $1$ and the products $E_i E_j = 0$ for $i \neq j$ ,then $E_i$ are idempotent linear transformations and $V$ can be expressed as the direct sum 
$$
V = E_1 V \oplus \ldots \oplus E_s V
$$














